---
layout: single
title: "LLM Pretraining for Scientific Sequences"
permalink: /research/llm-pretraining-scientific-sequences/
author_profile: false
section_icon: operator
research_area: true
research_order: 20
research_image: /images/thumbs/llm.svg
research_card_description: "Pretraining and adaptation for chemical languages and biological sequences (DNA/RNA/protein)."
research_tags:
  - pretraining
  - foundation models
  - scientific tokens
research_ticker: "LLM PRETRAINING • SCIENTIFIC SEQUENCES • TOKENS • CHEMISTRY • RNA • PROTEIN"
---

{% include base_path %}

<div class="cta-row" style="margin-top: 0.5rem;">
  <a class="btn" href="{{ base_path }}/#research"><i class="fa-solid fa-arrow-left" aria-hidden="true"></i><span>Back to Research</span></a>
  <a class="btn" href="{{ base_path }}/publications/"><i class="fa-solid fa-book-open" aria-hidden="true"></i><span>Publications</span></a>
</div>

## Overview

Pretraining and adaptation for chemical languages and biological sequences (DNA/RNA/protein).

## Topics

* pretraining
* foundation models
* scientific tokens

## Notes

Write your research summary here in Markdown.
